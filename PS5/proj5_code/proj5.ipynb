{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiK4QNWKDvWJ"
   },
   "outputs": [],
   "source": [
    "# Scene Recognition and Segmentation with Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8-TUO3hQhZ-2"
   },
   "source": [
    "## Outline\n",
    "\n",
    "In this project, we will use *convolutional neural nets* to classify images into different scenes, and perform segmentation of images.\n",
    "\n",
    "Basic learning objectives of this project:\n",
    "1. Construct the fundamental pipeline for performing deep learning using PyTorch;\n",
    "2. Understand the concepts behind different layers, optimizers.\n",
    "3. Experiment with different models and observe the performance.\n",
    "\n",
    "The starter code is mostly initialized to 'placeholder' just so that the starter\n",
    "code does not crash when run unmodified and you can get a preview of how\n",
    "results are presented.\n",
    "\n",
    "## Compute Requirements\n",
    "\n",
    "This classification part of the project is doable without a GPU, but a GPU makes the process much more faster and frustration free.\n",
    "\n",
    "The segmentation part of the project needs a GPU to make the process faster and feasible. We have provided a separate notebook `proj5_code/proj5_colab.ipynb` for training with Colab for Segmentation.\n",
    "\n",
    "You can try out Google Colab to run this notebook. These are the steps we follow:\n",
    "1. Upload this notebook to google colab\n",
    "2. Zip all the components in the project directory and upload it to the colab runtime\n",
    "3. Unzip the uploaded zip using ```!unzip -qq <uploaded_file>.zip -d ./```\n",
    "\n",
    "Remember to download all the files saved and changes to code you made on the colab.\n",
    "\n",
    "Note that we will not be actively supporting issues with Google colab. Please take help from fellow students and use the internet to solve the issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3KVmDW-TdjI"
   },
   "source": [
    "## Dataset\n",
    "You would have already stored the dataset in the ```data``` folder. It is the same data you have used in PS4. It has two subfolders: ```train``` and ```test```. Go through any of the folder and find there you will find the folders with scene names like *bedroom*, *forest*, *office*. These are the 15 scenes that we want our model to predict given an RGB image. You can look into folder for each scene to find multiple images. All this data is labelled data provided to you for training and testing your model.\n",
    "\n",
    "**Let's start coding now!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFVeSYKzTdjK"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QRsVzCGTdjB"
   },
   "outputs": [],
   "source": [
    "# uncomment for running on colab\n",
    "# !unzip -qq proj5_colab.zip -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kTYp67dhZ_D"
   },
   "outputs": [],
   "source": [
    "# flag to modify paths to run better on Colab; change it to true if you want to run on colab\n",
    "use_colab_paths = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DikW12-RhZ_J"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g1dqr6qSBpE2",
    "outputId": "c9459d34-35e8-4f37-f3bb-8487de6fcadf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from proj5_code.classification.runner import Trainer\n",
    "from proj5_code.classification.optimizer import compute_quadratic_loss, get_optimizer, gradient_descent_step\n",
    "from proj5_code.classification.simple_net import SimpleNet\n",
    "from proj5_code.classification.simple_net_dropout import SimpleNetDropout\n",
    "from proj5_code.classification.data_transforms import get_fundamental_transforms, get_data_augmentation_transforms\n",
    "from proj5_code.classification.stats_helper import compute_mean_and_std\n",
    "from proj5_code.classification.vis import visualize\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CG3e0869TdjS"
   },
   "outputs": [],
   "source": [
    "from proj5_code.proj5_unit_tests.test_base import verify\n",
    "from proj5_code.proj5_unit_tests.classification.test_stats_helper import test_mean_and_variance\n",
    "from proj5_code.proj5_unit_tests.classification.test_data_transforms import test_fundamental_transforms\n",
    "from proj5_code.proj5_unit_tests.classification.test_dl_utils import test_predict_labels, test_compute_loss\n",
    "from proj5_code.proj5_unit_tests.classification.test_simple_net import test_simple_net\n",
    "from proj5_code.proj5_unit_tests.classification.test_simple_net_dropout import test_simple_net_dropout\n",
    "from proj5_code.proj5_unit_tests.classification.test_checkpoints import test_simple_net_checkpoint\n",
    "from proj5_code.proj5_unit_tests.classification.test_optimizer import test_compute_quadratic_loss, test_gradient_descent_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjE0jIc5BpFN"
   },
   "outputs": [],
   "source": [
    "is_cuda = True\n",
    "# torch.cuda.is_available() checks if there is a GPU on the system and it has CUDA drivers installed\n",
    "# CUDA helps accelerate operations in Pytorch\n",
    "is_cuda = is_cuda and torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKRS4gvZTdji"
   },
   "outputs": [],
   "source": [
    "data_base_path = '../data/' if not use_colab_paths else 'data/'\n",
    "model_base_path = '../model_checkpoints/' if not use_colab_paths else 'model_checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "1QoVOsGwhZ_j",
    "outputId": "433432c2-bfe7-4d68-93bd-4579415578cf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(os.listdir(data_base_path))\n",
    "print(os.listdir(model_base_path))\n",
    "\n",
    "# TODO: check that these outputs are as per expectation. It will save a lot of time in debugging issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nulEn5fzTdjs"
   },
   "source": [
    "To train a network in PyTorch, we need 4 components:\n",
    "1. **Dataset** - an object which can load the data and labels given an index.\n",
    "2. **Model** - an object that contains the network architecture definition.\n",
    "3. **Loss function** - a function that measures how far the network output is from the ground truth label.\n",
    "4. **Optimizer** - an object that optimizes the network parameters to reduce the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA7pA3FWhZ_I"
   },
   "source": [
    "## 1 Dataset\n",
    "The dataset is in the ```data``` folder. It has two subfolders: ```train``` and ```test```. Go through any of the folder and find there you will find the folders with scene names like *bedroom*, *forest*, *office*. These are the 15 scenes that we want our model to predict given an RGB image. You can look into folder for each scene to find multiple images. All this data is labelled data provided to you for training and testing your model.\n",
    "\n",
    "**Let's start coding now!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B027mwlKTdjt"
   },
   "source": [
    "One crucial aspect of deep learning is to perform data preprocessing. In DL, we usually *normalize* the dataset and perform some *transformations* on them. The transformations can either help the inputs be compatible with the model (say our model only works on 500x500 images and we need all input to be cropped/scaled to this size) or help in data-augmentation to improve performance (more on this later).\n",
    "\n",
    "\n",
    "### 1.1 Compute mean and standard deviation of the dataset\n",
    "In this project we are going to \"zero-center\" and \"normalize\" the dataset so that each entry has zero mean and the overall standard deviation is 1. To do this, we will compute the mean and variance of the actual dataset first. To perform actual normalization, we will use a transform from Pytorch. \n",
    "\n",
    "**TODO 1**:  fill in the `compute_mean_and_std()` in `proj5_code/classification/stats_helper.py` to compute the **mean** and **standard deviation** of both training and test set combined.\n",
    "\n",
    "Debug tip: If you face an error from StandardScaler about attribute not found, please check that the paths are correct and your code actually finds some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "vWA_2UbjBpFd",
    "outputId": "c84201a5-d7c9-4e0e-9b42-5fb784034556"
   },
   "outputs": [],
   "source": [
    "print(\"Testing your mean and std computation: \", verify(test_mean_and_variance))\n",
    "dataset_mean, dataset_std = compute_mean_and_std(data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xixFr8CDBpFn",
    "outputId": "e5da02f4-ac00-4443-8144-4e4e72bb1555"
   },
   "outputs": [],
   "source": [
    "print('Dataset mean = {}, standard deviation = {}'.format(dataset_mean, dataset_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2TeGbrQBpFu"
   },
   "source": [
    "### 1.2 Data Loader\n",
    "\n",
    "We'll have stored our input images in a specific format: each class name is a folder, with images belonging to that class inside the folder. This specific choice of storage scheme enables us to directly use the [`ImageFolder` ](https://pytorch.org/vision/stable/datasets.html#imagefolder) dataset from Pytorch.\n",
    "\n",
    "We'll use the `ImageFolder` to make two loaders, for train and test split respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gp3y22Zi18cd"
   },
   "outputs": [],
   "source": [
    "train_loader = ImageFolder(root=os.path.join(data_base_path, 'train'))\n",
    "test_loader = ImageFolder(root=os.path.join(data_base_path, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGZEr-Bs18cd",
    "outputId": "f7a5b82c-8fdc-4b4e-a768-aeb4aa9cb6f5"
   },
   "outputs": [],
   "source": [
    "print(f\"Number of entries in training set: {len(train_loader)}\")\n",
    "print(f\"Number of entries in test set: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJ-AfOCS18ce"
   },
   "source": [
    "Visualizing some examples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_rOR0Oi18ce",
    "outputId": "74b58fa9-1603-4956-a1ce-c6701d76487e"
   },
   "outputs": [],
   "source": [
    "idx = 100\n",
    "\n",
    "image, class_label = train_loader.__getitem__(idx)\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.title(f'Image at index {idx}. Class Label {class_label}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYVDMxFL18ce",
    "outputId": "4d7deb59-eb38-4a20-9047-926f45cce42a"
   },
   "outputs": [],
   "source": [
    "idx = 300\n",
    "\n",
    "image, class_label = train_loader.__getitem__(idx)\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.title(f'Image at index {idx}. Class Label {class_label}')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EIq75D2CTdkC"
   },
   "source": [
    "### 1.3 Data transforms\n",
    "In this section, we will construct some fundamental transforms to process RGB images into torch tensors, which we can provide as input to our model.\n",
    "\n",
    "1. Resize the input image to the desired shape\n",
    "2. Convert to a single channel grayscale image (ImageFolder will read the image as a color image with 3 channels, even though the colors are all gray).\n",
    "3. Convert it to a tensor\n",
    "4. Normalize them based on the mean and standard deviation already computed in 1.1.\n",
    "\n",
    "All these operations can be done using [torchvision's transforms](https://pytorch.org/vision/stable/transforms.html). You'll need to write one line for each operation.\n",
    "\n",
    "**TODO 2:** For this part, complete the function `get_fundamental_transforms()` in `proj5_code/classification/data_transforms.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "ZoAaL11nTdkD",
    "outputId": "e95d0343-9107-462b-aebe-2f6990efdd08"
   },
   "outputs": [],
   "source": [
    "print(\"Testing your fundamental data transforms: \", verify(test_fundamental_transforms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sWOnZmNTdkJ"
   },
   "source": [
    "## 2 Model Architecture and Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "md0ni09P18cf"
   },
   "outputs": [],
   "source": [
    "inp_size = (64,64) # input size to use for this section"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bg59TFXHTdkL"
   },
   "source": [
    "### 2.1 SimpleNet Model\n",
    "\n",
    "The data is ready! Now we are preparing to move to the actual core of deep learning: the architecture. To get you started in this part, simply define a **2-layer** model in the `proj5_code/classification/simple_net.py`. Here by \"2 layers\" we mean **2 convolutional layers**, so you need to figure out the supporting utilities like ReLU, Max Pooling, and Fully Connected layers, and configure them with proper parameters (such as kernel size, padding, etc) such that the output from a layer is compatible as input to the next layer.\n",
    "\n",
    "You may refer the image *simplenet.jpg* in the base folder for a sample network architecture (it's the architecture TAs used in their implementation and is sufficient to get you pass Part 1). In that image, the layers in use are written at the bottom, and the blocks show the shape of features after each layer.\n",
    "\n",
    "Note that the input image will be shaped as (N, C, H, W) where N are the number of images in a batch, C is the number of color channels in the image, H is the height, and W is the width.\n",
    "\n",
    "**TODO 3**: Do the following in ```proj5_code/classification/simple_net.py```:\n",
    "- Initialize ```self.conv_layers```\n",
    "- Initialize ```self.fc_layers```\n",
    "- Write the forward function\n",
    "\n",
    "Leave the ```self.loss_criterion = None``` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jvVL-ap0BpFx",
    "outputId": "d97819e7-741e-4d37-82ea-8351dd103dff"
   },
   "outputs": [],
   "source": [
    "print(\"Testing your SimpleNet architecture: \", verify(test_simple_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_YLUulTTdkX"
   },
   "source": [
    "### 2.2 Output prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNBpN4ofTdkZ"
   },
   "source": [
    "Let's see what out model's forward function produces for a sample input, and how it relates to classification. Pytorch's convolution and FC layers are initialized with random weights. So we should not expect any useful output without any training.\n",
    "\n",
    "We will use a data-point from a new `ImageFolder` object with the transformations we have defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IW3f_SgTdkc"
   },
   "outputs": [],
   "source": [
    "simple_model = SimpleNet()\n",
    "\n",
    "train_loader = ImageFolder(root=os.path.join(data_base_path, 'train'), \n",
    "                           transform=get_fundamental_transforms(inp_size, dataset_mean, dataset_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyQDxArjTdkg"
   },
   "outputs": [],
   "source": [
    "# get the 0th sample\n",
    "sample_image, sample_label = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "myJ9kcqUTdkm",
    "outputId": "ec53cd08-979d-41d7-bc7b-07eca46ed064"
   },
   "outputs": [],
   "source": [
    "print('Input image shape = ', sample_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "bvHtwSATTdkr",
    "outputId": "ea02c24c-e16c-4af7-e3d1-5b8dc25cf280"
   },
   "outputs": [],
   "source": [
    "#show the image\n",
    "fig, axs = plt.subplots()\n",
    "axs.imshow(sample_image.squeeze().numpy(), cmap='gray')\n",
    "axs.axis('off')\n",
    "axs.set_title('Sample image (Target label = {})'.format(sample_label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0XEqQaNTdkw"
   },
   "outputs": [],
   "source": [
    "# run the image through the model\n",
    "sample_model_output = simple_model(sample_image.unsqueeze(0)).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "2v9WUSlbTdkz",
    "outputId": "965375e6-08dd-48da-c4cf-ce3ccef31e2c"
   },
   "outputs": [],
   "source": [
    "print(sample_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCtiLVKKTdk3"
   },
   "source": [
    "We have a 15-dimensional tensor as output, but how does it relate to classification?\n",
    "\n",
    "We first convert the this tensor into a probability distribution over 15 classes by applying the [Softmax](https://en.wikipedia.org/wiki/Softmax_function) operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ssu7W4jTdk4"
   },
   "outputs": [],
   "source": [
    "sample_probability_values = torch.nn.functional.softmax(sample_model_output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "4BXWrwl0Tdk9",
    "outputId": "eb041618-7ba1-4ded-ce43-5eb8a395ac86"
   },
   "outputs": [],
   "source": [
    "print(sample_probability_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8ax6_3y18ci",
    "outputId": "cc6fe3c7-b560-4291-9884-4f1725d95593"
   },
   "outputs": [],
   "source": [
    "print(\"Sum of probability values: \", torch.sum(sample_probability_values))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iTABRuGpTdlD"
   },
   "source": [
    "The prediction of the model will be the index where the probability distribution is the maximum. Convince yourself that the argmax-operation on *sample_model_values* is the same as the argmax-operation on *sample_probability_values*. Hence for prediction, we can take the argmax on the model output directly without applying a softmax. Also, you can check that the sum of softmax values is 1 and hence they are valid probability values.\n",
    "\n",
    "**TODO 4:** Complete the ```predict_labels()``` function in ```proj5_code/classification/dl_utils.py```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3j6LNUyTdlD"
   },
   "source": [
    "## 3 Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2mIs5sQTdlE"
   },
   "source": [
    "We have written a model which takes in a tensor for an image and produces a 15 dimensional output for it. We saw in the previous section on how the output relates to the prediction and probability distribution. But how do we quantify the performance of the model, and how do we use that quantification to form an objective function which we can minimize."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1esn_cCXTdlE"
   },
   "source": [
    "Ideally, we would want the probability function to have value 1 for the target *sample_label* and value 0 for the remaining class indices. To penalize the deviation between the desired probability distribution and the model-predicted distrtibution, we use the KL-divergence loss or the cross-entropy loss. Please refer to [this stackexchange post](https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation) for a good explanation and derivation.\n",
    "\n",
    "Note: nn.CrossEntropyLoss() performs softmax itself, but nn.KLDivLoss() does not. Also, for KL-divergence loss, the ground truth has to be a distribution, not a numerical class label. Keep these in mind for the next TODO."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SvSEgfOBTdlF"
   },
   "source": [
    "**TODO 5:** Assign a loss function to ```self.loss_criterion``` in ```proj5_code/classification/simple_net.py```. Note that we have not done a softmax operation in the model's forward function and choose the [appropriate loss function](https://pytorch.org/docs/stable/nn.html#loss-functions). In this TODO, you just have to assign the correct loss function in the model. \n",
    "\n",
    "**TODO 6:** Complete the ```compute_loss()``` function in ```proj5_code/classification/dl_utils.py``` to use the model's loss criterion and compute the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9Y3TEv7TdlG"
   },
   "outputs": [],
   "source": [
    "simple_model = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "Y2Lu0S_ETdlK",
    "outputId": "584fd9e2-1fbf-47c7-dc18-81e70ebfc2c3"
   },
   "outputs": [],
   "source": [
    "print(simple_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "hbcEPDN0TdmF",
    "outputId": "4545c9ed-1dcb-45fe-eaba-c51d21163bc1"
   },
   "outputs": [],
   "source": [
    "print(\"Testing your model prediction: \", verify(test_predict_labels))\n",
    "print(\"Testing your loss values: \", verify(test_compute_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKQCj596TdlQ"
   },
   "source": [
    "## 4 Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CB0oPQtVTdlR"
   },
   "source": [
    "### 4.1 Manual gradient descent using Pytorch's autograd\n",
    "\n",
    "Till now, we have defined the model, and designed a loss function which is a proxy for *good* classification. We now to optimize the weights of the network so that the loss function is minimized.\n",
    "\n",
    "Pytorch is a very useful library for deep learning because a lot of tensor operations and functions support the flow of gradients. This feature is called [autograd](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html). This functionality lets use use gradient based optimization techniques like gradient descent without writing a lot of code.\n",
    "\n",
    "Let us first understand how we can access the gradients.\n",
    "\n",
    "### Define an objective function (equivalent to the loss we defined in Section 3)\n",
    "Suppose we have a simple objective function that looks like:\n",
    "$$ L(w) =  w^2 - 10w + 25 $$\n",
    "\n",
    "This is a convex problem, and we know that the loss $L$ is minimized for $w=5$, and we can obtain this in closed form.\n",
    "\n",
    "But let us use gradient descent to obtain the solution in this case."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz7W4cWU18cm"
   },
   "source": [
    "**TODO 7:** Complete `compute_quadratic_loss` in `proj5_code/classification/optimizer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLu-7PG1TdlS",
    "outputId": "7d5a393e-fdb0-4039-d114-6c56c179be13"
   },
   "outputs": [],
   "source": [
    "print(\"Testing your quadratic loss function: \", verify(test_compute_quadratic_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPVWOjo6TdlW"
   },
   "source": [
    "Let's compute the loss at w = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pbVokXpUTdlX",
    "outputId": "666d20f0-39b9-4719-fe4c-338e2e117534"
   },
   "outputs": [],
   "source": [
    "w = torch.tensor([0.0], requires_grad=True) # we mark this tensor to require gradient so that the computation graph\n",
    "# can account for it\n",
    "\n",
    "loss = compute_quadratic_loss(w)\n",
    "\n",
    "print('w={:.4f}\\tLoss={:.4f}'.format(w.detach().numpy().item(), loss.detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AabZ-6MnTdla"
   },
   "source": [
    "Now we can do a backward pass of the gradients to get the gradient of loss w.r.t w. Now we need to calculate the gradients with regard to the weights and biases using backprop. It will be very painful if we do it manually, but thankfully, in PyTorch we can have everything covered with autograd, which only needs a simple call of **.backward()** on our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "jcrIh9P3Tdlb",
    "outputId": "2f5b1efe-ece3-433e-e583-3f3e204f3adf"
   },
   "outputs": [],
   "source": [
    "# perform a backward pass on loss (we need to retain graph here otherwise Pytorch will throw it away)\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "print(w.grad.data)\n",
    "\n",
    "# manually zero out the gradient\n",
    "w.grad.zero_()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAPF2cBATdlf"
   },
   "source": [
    "Does this gradient match with the one you compute manually?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmd3JnxzTdli"
   },
   "source": [
    "With the gradients, we can update the weights and biases using gradient descent:\n",
    "$$w_{k+1}=w_{k} - \\alpha\\frac{\\partial L}{\\partial w_k}$$\n",
    "where $w$ is the parameter we are updating, $\\alpha$ is the learning rate, and $\\frac{\\partial L}{\\partial w_k}$ is the gradient at step $k$. You can learn more about gradient descent [here](https://en.wikipedia.org/wiki/Gradient_descent) and [here](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO4CMb-A18co"
   },
   "source": [
    "**TODO 8:** Implement a single step of gradient descent in function `gradient_descent_step` in `proj5_code/classification/optimizer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHjijLvb18cp",
    "outputId": "06809aba-3871-40f3-d845-85564bdac4ac"
   },
   "outputs": [],
   "source": [
    "print(\"Testing gradient descent step: \", verify(test_gradient_descent_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMEZ9eHuTdlm"
   },
   "source": [
    "Let's take one step of the gradient descent and check if the the loss value decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BuBQHZwTdln"
   },
   "outputs": [],
   "source": [
    "# set learning rate\n",
    "lr = .03\n",
    "\n",
    "loss = compute_quadratic_loss(w)\n",
    "gradient_descent_step(w, loss, lr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "yIWP3Tv4Tdlq",
    "outputId": "ba32ccb8-73d9-4a1d-ab6f-4822df3eb4eb"
   },
   "outputs": [],
   "source": [
    "loss = compute_quadratic_loss(w)\n",
    "print('w={:.4f}\\tLoss={:.4f}'.format(w.detach().numpy().item(), loss.detach().numpy().item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Mb4IdocTdlv"
   },
   "source": [
    "Looks like it's been optimized!\n",
    "\n",
    "Now let's run a few more updates and see where we can get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "FTfmRlc0Tdlv",
    "outputId": "0b111b50-90d6-47a2-b870-c05c2fc1b88a"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "for i in range(200):\n",
    "    loss = compute_quadratic_loss(w)\n",
    "    if not (i+1)%10:\n",
    "        print('Iteration {}: w={:.4f}\\tLoss={:.4f}'.format(\n",
    "            i+1, w.detach().numpy().item(), loss.detach().numpy().item()))\n",
    "        \n",
    "    gradient_descent_step(w, loss, lr) \n",
    "        \n",
    "print('\\noptimization takes %0.3f seconds'%(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTFEo_JMTdlz"
   },
   "source": [
    "Seems that it's doing a great job training our model! The loss now has decreased significantly to a pretty small value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMtBy2JvTdl0"
   },
   "source": [
    "### 4.2 Optimization using Pytorch's gradient descent optimizer\n",
    "\n",
    "Now let's see how we can simplify this using the `torch.optim` package from PyTorch. You can see that using optimizer from `torch.optim` package can achieve the same results with a lot less code from our side. Also, there are many features available over the vanilla gradient descent. Let's use the Stochastic Gradient Descent (SGD) optimizer available in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "YKb3ngxtTdl1",
    "outputId": "b444730f-5b89-4ecb-c5c5-95a93833d851"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# define parameters we want to optimize\n",
    "w = torch.tensor([0.0], requires_grad=True)\n",
    "\n",
    "optimizer = SGD([w], lr=lr)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(200):\n",
    "    loss = compute_quadratic_loss(w)   \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if not (i+1)%10:\n",
    "        print('Iteration {}: w={:.4f}\\tLoss={:.4f}'.format(\n",
    "            i+1, w.detach().numpy().item(), loss.detach().numpy().item()))\n",
    "        \n",
    "print('\\noptimization takes %0.3f seconds'%(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IsnddUCTdl5"
   },
   "source": [
    "### 4.3 Setting up the optimizer for SimpleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxeVYy1bTdl6"
   },
   "source": [
    "**TODO 9:** **initialize the following cell with proper values for learning rate and weight decay** \n",
    "\n",
    "**Note:** There is nothing to do in this TODO for the first pass. You'll train the model with these values and it will be bad. Then you can come back here and tune the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2cwtK5PBpF7"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"adam\",\n",
    "  \"lr\": 1,\n",
    "  \"weight_decay\": 1e-2\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3pyzj8KTdl_"
   },
   "source": [
    "We will now set up a utility function to define an optimizer on the loss for a model.\n",
    "\n",
    "**TODO 10:** complete the ```get_optimizer()``` function in ```proj5_code/classification/optimizer.py```. The helper function accepts three basic configurations as defined below. Any other configuration is optional. *SGD* optimizer type should be supported, anything else is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0CrYZa4BpGE"
   },
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(simple_model, optimizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b8cjmrSTdmK"
   },
   "source": [
    "## 5 Training SimpleNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBrJnj2tTdmL"
   },
   "source": [
    "We have completed all the components required to train the first model for this course. Let's pass in the model architecture, optimizer, transforms for both the training and testing datasets into the trainer, and proceed to the next cell to train it. If you have implemented everything correctly, you should be seeing a decreasing loss value.\n",
    "\n",
    "**Note** in this project, we will be using the test set as the validation set (i.e. using it to guide our decisions about models and hyperparamters while training). In actual practise, you would not interact with the test set until reporting the final results.\n",
    "\n",
    "**Note** that your CPU should be sufficient to handle the training process for all networks in this project, and the following training cells will take less than 5 minutes; you may also want to decrease the value for `num_epochs` and quickly experiment with your parameters. The default value of **30** is good enough to get you around the threshold for Part 1, and you are free to increase it a bit and adjust other parameters in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiGOvPJfBpGO"
   },
   "outputs": [],
   "source": [
    "# re-init the model so that the weights are all random\n",
    "simple_model = SimpleNet()\n",
    "optimizer = get_optimizer(simple_model, optimizer_config)\n",
    "\n",
    "trainer = Trainer(data_dir=data_base_path, \n",
    "                  model = simple_model,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = os.path.join(model_base_path, 'simple_net'),\n",
    "                  train_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "paNLyU5cBpGX",
    "outputId": "e8506469-b4e5-4aa6-9423-8235b96068ab"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "simple_net_start = time.time()\n",
    "trainer.train(num_epochs=30)\n",
    "simple_net_end = time.time()\n",
    "print(\"The training time taken for simple net is {:.9f}\".format(simple_net_end-simple_net_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jv1T8xv2TdmR"
   },
   "source": [
    "Now let's have your model predict on some examples and see how well it performs qualitatively. You should try the following cell multiple times to understand whats happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thCGob3JTdmR"
   },
   "outputs": [],
   "source": [
    "# visualize train split\n",
    "print(\"Examples from train split:\")\n",
    "visualize(simple_model, 'train', get_fundamental_transforms(inp_size, dataset_mean, dataset_std), data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "QQbkZhjlTdmU",
    "outputId": "d227b903-2c31-4a5e-b3b2-8ade627c0791"
   },
   "outputs": [],
   "source": [
    "# visualize test split\n",
    "print(\"Examples from test split:\")\n",
    "visualize(simple_model, 'test', get_fundamental_transforms(inp_size, dataset_mean, dataset_std), data_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "z0b_WwJhBpGf",
    "outputId": "3d80145a-dd14-40fd-f30c-9f47ac0f4d75",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "8epn0IBmBpGn",
    "outputId": "23555727-ed23-490d-ddfc-783fa439101d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {:.4f}; Validation Accuracy = {:.4f}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "CQKKbNc2haCZ",
    "outputId": "8ef542c8-409c-4ed2-840f-198a711e66e1"
   },
   "outputs": [],
   "source": [
    "print('Testing simple net weights saved: ', verify(test_simple_net_checkpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWNCMmRzTdmc"
   },
   "source": [
    "After you have finished the training process, now plot out the loss and accuracy history. You can also check out the final accuracy for both training and validation data. Copy the accuracy plots and values onto the report, and answer the questions there. \n",
    "\n",
    "**TODO 11:** Obtain a **45%** validation accuracy to receive full credits for Part 1. You can go back to TODO 8 first to tune your paramters for optimization using the following tips:\n",
    "\n",
    "**Tips**:\n",
    "1. If the loss decreases very slowly, try increasing the value of the lr (learning rate).\n",
    "2. Initially keep the value of weight decay (L2-regularization) very low. Even zero is fine. Use the weight decay as the last resort.\n",
    "3. Try to first adjust lr in multiples of 3 initially. When you are close to reasonable performance, do a more granular adjustment. Do try to keep lr values less than 0.1.\n",
    "4. If you want to increase the validation accuracy by a little bit, try increasing the weight_decay to prevent overfitting. Do not use tricks from Section 6 just yet.\n",
    "\n",
    "If you still need to tweak the model architecture, you are free to do so. But remember complex models will require more time to train, and TAs could achieve ~50% accuracy with the described model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "G7hE1vidBpGt"
   },
   "source": [
    "## EC1 Solving overfitting\n",
    "We have obtained a 45% accuracy on the validation data with a simple model; Feeling even better for the training accuracy right? More than 90% (if you have implemented everything correctly). But should you?\n",
    "\n",
    "Our final accuracies for training and validation data differ a lot from each other, which indicates that the model we have defined **fits too well with the training data, but is unable to generalize well on data it has not trained on**: this is often regarded as **overfitting**. In this section we are going to apply 2 techniques to tackle with it: adjusting both data and model.\n",
    "\n",
    "References:\n",
    "- https://www.cs.princeton.edu/courses/archive/spring16/cos495/slides/ML_basics_lecture6_overfitting.pdf\n",
    "\n",
    "### EC1.1 Jitter, Random Flip, and Normalization\n",
    "One common technique to increase the \"variability\" of the data is to **augment** it. Firstly, we don't have a huge amount of data, so let's \"jitter\" it; secondly, when you mirror an image of a **kitchen**, you can tell that the mirrored image is still a kitchen. \n",
    "\n",
    "**TODO 12:** finish the `get_data_augmentation_transforms()` function in `proj5_code/classification/data_transforms.py`: you should first copy your existing fundamental transform implementation into this function, and then insert other which help you do the above adjustment before the fundamental ones.\n",
    "\n",
    "Tips: Try jittering the colors, and flipping the image horizontaly. These two operations wont change the scene contents. You can also try resizing and cropping. \n",
    "\n",
    "You are free to experiment with different kinds of augmentation techniques by adding new techniques or replacing existing techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ech4Y22OXOui"
   },
   "outputs": [],
   "source": [
    "inp_size = (64,64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "06cXE2wkTdmg"
   },
   "source": [
    "### EC1.2 Dropout\n",
    "\"Dropout\" is a technique commonly used to regularize the network. It randomly turns off the connection between neurons inside the network and prevent the network from relying too much on a specific neuron. \n",
    "\n",
    "References: \n",
    "- https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "- https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/\n",
    "\n",
    "**TODO 13:** finish the `proj5_code/classification/simple_net_dropout.py` with your previous SimpleNet model, plus the dropout layer. Think where you want the dropout should be places: fully connected layers, conv layers, or both? Generally the layer with more number of parameters might be more prone to overfitting.\n",
    "\n",
    "If you are skipping this part, copy over the exact simple net arhcitecute. You will fail the following unit test but you can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "adEPCL3QTdmi",
    "outputId": "370d27dc-8c72-4f4f-af1f-172c0a68cd15"
   },
   "outputs": [],
   "source": [
    "print(\"Testing your SimpleNetDropout architecture: \", verify(test_simple_net_dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "id": "nlgm5eM-BpGz",
    "outputId": "d0cf7323-6648-4c3c-9410-09d04e76c40a"
   },
   "outputs": [],
   "source": [
    "simple_model_dropout = SimpleNetDropout()\n",
    "print(simple_model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmmpMDRBTdmm"
   },
   "source": [
    "Similar to the previous part, **initialize the following cell with proper values for learning rate and weight decay**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btKIvIrdBpG5"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"adam\",\n",
    "  \"lr\": 1,\n",
    "  \"weight_decay\": 1e-2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExoLylurBpHH"
   },
   "outputs": [],
   "source": [
    "simple_model_dropout = SimpleNetDropout()\n",
    "optimizer = get_optimizer(simple_model_dropout, optimizer_config)\n",
    "\n",
    "trainer = Trainer(data_dir=data_base_path, \n",
    "                  model = simple_model_dropout,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = os.path.join(model_base_path, 'simple_net_dropout'),\n",
    "                  train_data_transforms = get_data_augmentation_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq2__3ZQTdmt"
   },
   "source": [
    "The following cell will take longer than Part 1, as now we have more data (and more variability), and the model is slightly more complicated than before as well; however, it should finish within 10~15 minutes anyway, and the default `num_epochs` is also good enough as a starting point for you to pass this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "id": "-ljUl4UnBpHN",
    "outputId": "8f6c2e23-917b-483e-a4c5-01015a84d480",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3yOROVGTdmy"
   },
   "source": [
    "Now let's have your model predict on some examples and see how well it performs qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "732LGRJCftzL",
    "outputId": "7fd5839e-acdb-4fee-e070-24d2641b3c89"
   },
   "outputs": [],
   "source": [
    "# # visualize train split\n",
    "print(\"Examples from train split:\")\n",
    "visualize(simple_model_dropout, 'train', get_fundamental_transforms(inp_size, dataset_mean, dataset_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jum7rI93Tdm0",
    "outputId": "297ad8c6-da18-4214-d353-889942f88608"
   },
   "outputs": [],
   "source": [
    "# # visualize test split\n",
    "print(\"Examples from test split:\")\n",
    "visualize(simple_model_dropout, 'test', get_fundamental_transforms(inp_size, dataset_mean, dataset_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "Gdh9AvHIBpHW",
    "outputId": "95f03c53-13c8-41b3-bb40-0da8262cb685",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "6SLuc3zmBpHd",
    "outputId": "095b6b08-2d99-4d85-a1f9-4273bd1546b7"
   },
   "outputs": [],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {:.4f}; Validation Accuracy = {:.4f}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBLf3n6aTdnJ"
   },
   "source": [
    "Similar to the previous part, now plot out the loss and accuracy history. Also copy the plots onto the report, and answer the questions accordingly.\n",
    "\n",
    "**TODO 14:** Achieve **52%** validation accuracy for full credits for this part."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Semantic Segmentation with Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1: A Simple Segmentation Baseline\n",
    "We'll start with a very simple baseline -- a pretrained ResNet-50, without the final averagepool/fc layer, and a single 1x1 conv as a final classifier, converting the (2048,7,7) feature map to scores over 11 classes, a (11,7,7) tensor. Note that our output is just 7x7, which is very low resolution. \n",
    "\n",
    "**TODO 15:** Implement upsampling to the original height and width, and compute the loss and predicted class per pixel in `proj_5_code/segmentation/simple_segmentation_net.py`. The loss function has already been set in the SimpleSegmentationNet class (`self.criterion`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from proj5_code.proj5_unit_tests.segmentation.test_simple_segmentation_net import (\n",
    "    test_check_output_shapes,\n",
    "    test_check_output_shapes_testtime,\n",
    ")\n",
    "test_check_output_shapes()\n",
    "\n",
    "print(\"test_check_output_shapes(): \", verify(test_check_output_shapes))\n",
    "print(\"test_check_output_shapes_testtime(): \", verify(test_check_output_shapes_testtime))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 16:** Now that you've completed your implementtion of SimpleSegmentationNet, train the network using the `proj5_code/proj5_colab.ipynb` notebook. Achieve at least 40% mIoU in order to achieve full credit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EC 2: PSPNet and ResNet-50\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSPNet and ResNet-50\n",
    "\n",
    "We'll be implementing PSPNet for this project, which uses a ResNet-50 backbone. First, we will provide some background information to familiarize yourself with the network and architecture.\n",
    "\n",
    "ResNet-50 has 50 convolutional layers, which is significantly deeper than your SimpleNet of Project 5. We give you the implementation in `proj5_code/segmentation/resnet.py`. \n",
    "\n",
    "The ResNet-50 is composed of 4 different sections (each called a \"layer\"), named `layer1`, `layer2`, `layer3`, `layer4`. Each layer is composed of a repeated number of blocks, and each such block is named a `BottleNeck`. Specifically, `layer1` has 3 Bottlenecks, `layer2` has 4 Bottlenecks, `layer3` has 6 Bottlenecks, and `layer4` has 3 Bottlenecks. In all, ResNet-50 has 16 Bottlenecks, which accounts for 48 of the conv layers.\n",
    "\n",
    "### Visualizing a ResNet Bottleneck Module\n",
    "\n",
    "The BottleNeck has a residual connection, from which ResNet gets its name:\n",
    "\n",
    "<img width=\"300\" src=\"https://user-images.githubusercontent.com/16724970/114430171-2ac1c200-9b8c-11eb-8341-fc943ff0945f.png\">\n",
    "\n",
    "See Figure 5 of the [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Bottleneck\n",
    "\n",
    "The Bottleneck is implemented exactly as the figure above shows, with 1x1 Conv -> BN -> ReLU -> 3x3 Conv -> BN -> ReLU -> 1x1 Conv -> BN -> Optional Downsample -> Add Back Input -> ReLU. The channel dimension of the feature map will be expanded by 4x, as we can see by the conv layer `in_features` and `out_features` parameters. And notice that the stride is set at the `conv2` module, which will be very important later.\n",
    "\n",
    "```python\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "```\n",
    "\n",
    "and the forward method of the `Bottleneck` shows the residual connection. Notice that when we add back the input (the identity operation), we may need to downsample it for the shapes to match during the add operation (if the main branch downsampled the input):\n",
    "```python\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Architecture\n",
    "Plotting the whole network architecture would require a massive figure, but we can show how data flows through just one Bottleneck, starting with 64 channels, and ending up with 256 output channels:\n",
    "<p float=\"left\">\n",
    "  <img src=\"https://user-images.githubusercontent.com/16724970/114427960-9eae9b00-9b89-11eb-9a3b-96817f205f32.png\" width=\"400\" />\n",
    "</p>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 17:** The first part of the PSPNet is to initialize the Resnet-50 backbone in `proj5_code/segmentation/pspnet.py`, specifically in the `__init__` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: Pyramid Pooling Module (PPM)\n",
    "One major component of the PSPNet is the PPM Module. After feeding an image through the ResNet backbone and obtaining a feature map, PSPNet aggregates context over different portions of the image with the PPM.\n",
    "\n",
    "The PPM splits the $H \\times W$ feature map into KxK grids. Here, 1x1, 2x2, 3x3,and 6x6 grids are formed, and features are average-pooled within each grid cell. Afterwards, the 1x1, 2x2, 3x3, and 6x6 grids are upsampled back to the original $H \\times W$ feature map resolution, and are stacked together along the channel dimension. These grids are visualized below (center):\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/16724970/114436422-4b414a80-9b93-11eb-8f02-8e7506b5f9a1.jpg\" width=\"900\">\n",
    "\n",
    "We have ALREADY implemented this for you in `proj5_code/segmentation/ppm.py`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 18:** Initialize the PPM Module in `proj5_code/segmentation/pspnet.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net Surgery for Increased Output Resolution and Receptive Field\n",
    "The basic ResNet-50 has two major problems:\n",
    "1. It does not have a large enough receptive field\n",
    "2. If run fully-convolutionally, it produces a low-resolution output (just $7 \\times 7$)!\n",
    "\n",
    "To fix the first problem, will need to replace some of its convolutional layers with dilated convolution. To fix the second problem, we'll reduce the stride of the network from 2 to 1, so that we don't downsample so much. Instead of going down to 7x7, we'll reduce to 28x28 for 224x224 input, or 26x26 for 201x201, like we do in this project. In other words, the downsampling rate will go from (1/32) to just (1/8).\n",
    "\n",
    "These animations depict how the dilated convolution (i.e. with dilation > 1) operation compares to convolution with no dilation (i.e. with dilation=1).\n",
    "\n",
    "Conv w/ Stride=1, Dilation=1 | Conv w/ Stride=2, Dilation=1 | Conv w/ Stride=1, Dilation=2\n",
    ":-: | :-: | :-:\n",
    "<img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" width=\"300\" align=\"center\"> | <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif\" width=\"300\" align=\"center\"> | <img src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/dilation.gif\" width=\"300\" align=\"center\"> \n",
    "\n",
    "\n",
    "In Layer3, in every `Bottleneck`, we will change the 3x3 `conv2`, we will replace the conv layer that had stride=2, dilation=1, and padding=1 with a new conv layer, that instead  has stride=1, dilation=2, and padding=2. In the `downsample` block, we'll also need to hardcode the stride to 1, instead of 2.\n",
    "\n",
    "In Layer4, for every `Bottleneck`, we will make the same changes, except we'll change the dilation to 4 and padding to 4.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 19:** Complete the rest of the function in `proj5_code/segmentation/pspnet.py`:\n",
    "\n",
    "- `__replace_conv_with_dilated_conv`\n",
    "- `__create_classifier`\n",
    "- `forward`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj5_code.proj5_unit_tests.segmentation.test_pspnet import (\n",
    "    test_pspnet_output_shapes,\n",
    "    test_check_output_shapes_testtime_pspnet,\n",
    "    test_pspnet_output_with_zoom_factor\n",
    ")\n",
    "\n",
    "print(\"test_pspnet_output_shapes():\", verify(test_pspnet_output_shapes))\n",
    "print(\"test_check_output_shapes_testtime_pspnet(): \", verify(test_check_output_shapes_testtime_pspnet))\n",
    "print(\"test_pspnet_output_with_zoom_factor(): \", verify(test_pspnet_output_with_zoom_factor))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO 20:** Now that you've completed your implementtion of PSPNet, train the network using the `proj5_code/proj5_colab.ipynb` notebook. Achieve at least 60% mIoU in order to achieve full extra credit."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "proj5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "069982490a6f40c99dd18422032d2c88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92f6efa8b5fd49e69bdada865cbc3fab",
      "max": 244418560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6ecbfe7ee7b4d4da398c5ee56badae9",
      "value": 244418560
     }
    },
    "0b81f9c1f6bc454a87fe4778bf952649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_069982490a6f40c99dd18422032d2c88",
       "IPY_MODEL_5cb5fa8ce08046aab383a70a67501a4b"
      ],
      "layout": "IPY_MODEL_fd2dd89a02c845749277122b6b6ff950"
     }
    },
    "5cb5fa8ce08046aab383a70a67501a4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88ea6259ff60465884b7d21ee6b48479",
      "placeholder": "",
      "style": "IPY_MODEL_c6638c5846de4196a0a8238b10db6cdd",
      "value": " 233M/233M [01:46&lt;00:00, 2.30MB/s]"
     }
    },
    "88ea6259ff60465884b7d21ee6b48479": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92f6efa8b5fd49e69bdada865cbc3fab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6638c5846de4196a0a8238b10db6cdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6ecbfe7ee7b4d4da398c5ee56badae9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "fd2dd89a02c845749277122b6b6ff950": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
